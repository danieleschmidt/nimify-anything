name: ‚ö° Performance Monitoring & Regression Detection

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: "3.10"
  BENCHMARK_THRESHOLD: "10"  # 10% performance regression threshold

jobs:
  performance-baseline:
    name: üìä Performance Baseline
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        # Fetch full history for performance comparison
        fetch-depth: 0
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-perf-${{ hashFiles('**/pyproject.toml') }}
        
    - name: Install performance testing dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark memory-profiler psutil matplotlib
        
    - name: Create performance test environment
      run: |
        mkdir -p performance-results
        python -c "
        import json
        import datetime
        metadata = {
          'timestamp': datetime.datetime.now().isoformat(),
          'commit': '${{ github.sha }}',
          'branch': '${{ github.ref_name }}',
          'python_version': '${{ env.PYTHON_VERSION }}',
          'runner_os': '${{ runner.os }}'
        }
        with open('performance-results/metadata.json', 'w') as f:
          json.dump(metadata, f, indent=2)
        "
        
    - name: Run core performance benchmarks
      run: |
        pytest tests/performance/ \
          --benchmark-json=performance-results/core-benchmarks.json \
          --benchmark-compare-fail=mean:${{ env.BENCHMARK_THRESHOLD }}% \
          --benchmark-histogram=performance-results/histogram \
          --benchmark-sort=mean
          
    - name: Run memory profiling
      run: |
        python -m memory_profiler tests/performance/test_memory_usage.py > performance-results/memory-profile.txt
        
    - name: Run load testing simulation
      run: |
        python tests/performance/test_load.py \
          --output performance-results/load-test-results.json \
          --duration 30 \
          --concurrent-users 10
          
    - name: Generate performance report
      run: |
        python scripts/compare-benchmarks.py \
          --current performance-results/core-benchmarks.json \
          --baseline performance-results/baseline-benchmarks.json \
          --output performance-results/comparison-report.html \
          --threshold ${{ env.BENCHMARK_THRESHOLD }}
        
    - name: Extract performance metrics
      id: metrics
      run: |
        python -c "
        import json
        
        # Load benchmark results
        with open('performance-results/core-benchmarks.json') as f:
          benchmarks = json.load(f)
        
        # Extract key metrics
        total_tests = len(benchmarks['benchmarks'])
        avg_time = sum(b['stats']['mean'] for b in benchmarks['benchmarks']) / total_tests
        
        print(f'TOTAL_TESTS={total_tests}')
        print(f'AVG_EXECUTION_TIME={avg_time:.4f}')
        
        # Check for regressions
        regressions = []
        for benchmark in benchmarks['benchmarks']:
          if 'compare' in benchmark and benchmark['compare']['change'] > 0.1:
            regressions.append(benchmark['name'])
        
        print(f'REGRESSIONS={len(regressions)}')
        print(f'REGRESSION_TESTS={\"|\".join(regressions)}')
        " >> $GITHUB_OUTPUT
        
    - name: Upload performance artifacts
      uses: actions/upload-artifact@v3
      with:
        name: performance-results-${{ github.sha }}
        path: performance-results/
        retention-days: 30
        
    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          // Read benchmark results
          const benchmarks = JSON.parse(fs.readFileSync('performance-results/core-benchmarks.json', 'utf8'));
          const metadata = JSON.parse(fs.readFileSync('performance-results/metadata.json', 'utf8'));
          
          // Format results
          let comment = `## ‚ö° Performance Test Results\n\n`;
          comment += `**Commit:** \`${metadata.commit.substring(0, 8)}\`\n`;
          comment += `**Total Tests:** ${benchmarks.benchmarks.length}\n\n`;
          
          comment += `### üìä Benchmark Summary\n\n`;
          comment += `| Test | Mean Time (s) | Std Dev | Min | Max |\n`;
          comment += `|------|---------------|---------|-----|-----|\n`;
          
          benchmarks.benchmarks.slice(0, 10).forEach(benchmark => {
            const stats = benchmark.stats;
            comment += `| ${benchmark.name} | ${stats.mean.toFixed(4)} | ${stats.stddev.toFixed(4)} | ${stats.min.toFixed(4)} | ${stats.max.toFixed(4)} |\n`;
          });
          
          if (benchmarks.benchmarks.length > 10) {
            comment += `\n*Showing top 10 results. Full results available in artifacts.*\n`;
          }
          
          // Add regression warnings
          const regressions = benchmarks.benchmarks.filter(b => 
            b.compare && b.compare.change > 0.1
          );
          
          if (regressions.length > 0) {
            comment += `\n### ‚ö†Ô∏è Performance Regressions Detected\n\n`;
            regressions.forEach(reg => {
              comment += `- **${reg.name}**: ${(reg.compare.change * 100).toFixed(1)}% slower\n`;
            });
          } else {
            comment += `\n### ‚úÖ No Performance Regressions Detected\n`;
          }
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  performance-regression-check:
    name: üö® Regression Detection
    runs-on: ubuntu-latest
    needs: performance-baseline
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Download current performance results
      uses: actions/download-artifact@v3
      with:
        name: performance-results-${{ github.sha }}
        path: current-results/
        
    - name: Download baseline performance results
      uses: dawidd6/action-download-artifact@v2
      with:
        workflow: performance-monitoring.yml
        branch: main
        name: performance-results-baseline
        path: baseline-results/
        if_no_artifact_found: warn
        
    - name: Compare performance metrics
      id: compare
      run: |
        if [ -f "baseline-results/core-benchmarks.json" ]; then
          python scripts/compare-benchmarks.py \
            --current current-results/core-benchmarks.json \
            --baseline baseline-results/core-benchmarks.json \
            --threshold ${{ env.BENCHMARK_THRESHOLD }} \
            --output comparison-report.json
          
          # Check if regressions were found
          REGRESSIONS=$(jq '.regressions | length' comparison-report.json)
          echo "REGRESSION_COUNT=$REGRESSIONS" >> $GITHUB_OUTPUT
          
          if [ "$REGRESSIONS" -gt 0 ]; then
            echo "PERFORMANCE_STATUS=failed" >> $GITHUB_OUTPUT
            echo "Performance regressions detected!"
            jq '.regressions' comparison-report.json
          else
            echo "PERFORMANCE_STATUS=passed" >> $GITHUB_OUTPUT
            echo "No performance regressions detected."
          fi
        else
          echo "PERFORMANCE_STATUS=baseline_missing" >> $GITHUB_OUTPUT
          echo "No baseline found, marking as informational."
        fi
        
    - name: Fail on performance regression
      if: steps.compare.outputs.PERFORMANCE_STATUS == 'failed'
      run: |
        echo "‚ùå Performance regression detected!"
        echo "This PR introduces performance regressions that exceed the ${{ env.BENCHMARK_THRESHOLD }}% threshold."
        echo "Please review and optimize the changes before merging."
        exit 1

  store-baseline:
    name: üíæ Store Performance Baseline
    runs-on: ubuntu-latest
    needs: performance-baseline
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - name: Download performance results
      uses: actions/download-artifact@v3
      with:
        name: performance-results-${{ github.sha }}
        path: performance-results/
        
    - name: Store as baseline
      uses: actions/upload-artifact@v3
      with:
        name: performance-results-baseline
        path: performance-results/
        retention-days: 90

  performance-monitoring:
    name: üìà Long-term Performance Monitoring
    runs-on: ubuntu-latest
    needs: performance-baseline
    if: github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download performance results
      uses: actions/download-artifact@v3
      with:
        name: performance-results-${{ github.sha }}
        path: performance-results/
        
    - name: Generate performance trend report
      run: |
        python scripts/performance-trends.py \
          --input performance-results/ \
          --output performance-trends.html \
          --lookback-days 30
          
    - name: Upload trend report
      uses: actions/upload-artifact@v3
      with:
        name: performance-trends-${{ github.run_number }}
        path: performance-trends.html
        
    - name: Check for performance degradation
      run: |
        python scripts/performance-health-check.py \
          --results performance-results/core-benchmarks.json \
          --alert-threshold 20 \
          --email-endpoint ${{ secrets.PERFORMANCE_ALERT_EMAIL }}